# WebScraper + Vectorizer - UniPost API

Sistema completo de **Web Scraping Inteligente** e **Vetoriza√ß√£o Autom√°tica** utilizando MCP SDK Python e Google Gemini.

## ü§ñ Funcionalidades

### WebScraper (`webscraper.py`)
- **Consulta sites de refer√™ncia** atrav√©s da API do projeto (modelo Sites)
- **Scraping automatizado** usando MCP SDK gratuito
- **Salva resultados em JSON** com timestamp e metadados
- **Sistema de logging completo** e tratamento robusto de erros
- **Autentica automaticamente** na API usando credenciais do Django

### Text Vectorizer (`text_vectorizer.py`)
- **Processa JSONs** gerados pelo webscraper automaticamente
- **Gera embeddings** usando Google Gemini (`embedding-001`)
- **Salva vetores** no modelo `Embedding` com `origin="webscraping"`
- **Remove JSONs** ap√≥s processamento bem-sucedido
- **Chunking inteligente** para textos grandes
- **Controle de duplicatas** e sistema de logs avan√ßado

## üìÅ Estrutura

```
scraping/
‚îú‚îÄ‚îÄ __init__.py                 # M√≥dulo Python
‚îú‚îÄ‚îÄ webscraper.py              # ‚ú® Web scraping com MCP SDK
‚îú‚îÄ‚îÄ text_vectorizer.py         # ‚ú® Vetoriza√ß√£o autom√°tica com Gemini
‚îú‚îÄ‚îÄ scraping.log              # Log do webscraper
‚îú‚îÄ‚îÄ vectorizer.log            # Log do vectorizer
‚îú‚îÄ‚îÄ scraping_results_*.json   # üóÇÔ∏è Resultados tempor√°rios (removidos ap√≥s vetoriza√ß√£o)
‚îî‚îÄ‚îÄ README.md                 # Esta documenta√ß√£o
```

## üöÄ Instala√ß√£o e Configura√ß√£o

### 1. Depend√™ncias j√° inclu√≠das

As depend√™ncias est√£o no `requirements.txt` principal:
```bash
# J√° inclu√≠dos no projeto
httpx==0.27.2              # Cliente HTTP ass√≠ncrono
mcp==1.1.0                 # MCP SDK para web scraping  
google-generativeai==0.8.3 # Google Gemini para embeddings
```

### 2. Vari√°veis de ambiente no `.env`:

```env
# Credenciais da API (obrigat√≥rio)
DJANGO_SUPERUSER_USERNAME=seu_usuario
DJANGO_SUPERUSER_PASSWORD=sua_senha
API_BASE_URL=http://localhost:8000/api/v1

# Google Gemini API Key (obrigat√≥rio para vectorizer)
GOOGLE_GEMINI_API_KEY=sua_chave_gemini
```

### 3. Obter chave do Google Gemini

1. Acesse [Google AI Studio](https://aistudio.google.com/)
2. Crie uma conta gratuita
3. Gere uma API Key
4. Adicione no `.env`

## üìã Como Usar

### Fluxo Completo (Recomendado)

```bash
# 1. Execute o web scraping
python scraping/webscraper.py

# 2. Processe e vetorize os dados coletados
python scraping/text_vectorizer.py
```

### Execu√ß√£o Individual

```bash
# Apenas web scraping (gera JSONs)
python scraping/webscraper.py

# Apenas vetoriza√ß√£o (processa JSONs existentes)  
python scraping/text_vectorizer.py
```

## üîÑ Fluxo Detalhado

### 1. WebScraper (`webscraper.py`)

```mermaid
graph LR
    A[Autentica na API] --> B[Obt√©m lista de Sites]
    B --> C[Inicializa MCP Client]
    C --> D[Scraping de cada site]
    D --> E[Salva JSON com timestamp]
```

**Processo:**
1. **Autentica√ß√£o**: Login na API com credenciais do Django
2. **Consulta Sites**: Busca lista de sites no endpoint `/api/v1/sites/`
3. **MCP Client**: Inicializa cliente MCP gratuito
4. **Web Scraping**: Realiza scraping usando web search
5. **Salva JSON**: Arquivo `scraping_results_YYYYMMDD_HHMMSS.json`

### 2. Text Vectorizer (`text_vectorizer.py`)

```mermaid
graph LR
    A[Encontra JSONs] --> B[Carrega dados]
    B --> C[Gera embeddings]
    C --> D[Salva no banco]
    D --> E[Remove JSONs]
```

**Processo:**
1. **Busca JSONs**: Localiza `scraping_results_*.json`
2. **Processa textos**: Extrai conte√∫do e aplica chunking se necess√°rio
3. **Gera embeddings**: Usa Google Gemini para vetoriza√ß√£o
4. **Salva banco**: Cria registros no modelo `Embedding`
5. **Cleanup**: Remove JSONs processados com sucesso

## üìä Dados e Metadados

### JSON de Scraping (Tempor√°rio)
```json
[
  {
    "site_name": "TechCrunch",
    "site_url": "https://techcrunch.com",
    "scraped_at": "2025-01-01T10:00:00Z",
    "content": "Conte√∫do do site...",
    "status": "success"
  }
]
```

### Embedding salvo no banco
```python
{
  "id": "uuid-gerado",
  "origin": "webscraping",
  "content": "Conte√∫do do site...",
  "title": "TechCrunch",
  "embedding_vector": [0.123, -0.456, ...], # 1536 dimens√µes
  "metadata": {
    "site_name": "TechCrunch",
    "site_url": "https://techcrunch.com", 
    "category": "NOTICIAS",
    "scraped_at": "2025-01-01T10:00:00Z",
    "chunk_index": 0,
    "total_chunks": 1,
    "processed_at": "2025-01-01T10:05:00Z"
  }
}
```

## üîç Monitoramento e Logs

### Logs Principais

```bash
# Log do web scraping
tail -f scraping/scraping.log

# Log da vetoriza√ß√£o  
tail -f scraping/vectorizer.log

# Logs em tempo real
tail -f scraping/*.log
```

### Verificar Resultados no Banco

```bash
python manage.py shell

>>> from embeddings.models import Embedding
>>> # Contar embeddings de web scraping
>>> Embedding.objects.filter(origin='webscraping').count()
42

>>> # Ver √∫ltimo embedding criado
>>> Embedding.objects.filter(origin='webscraping').latest('created_at')
<Embedding: webscraping: TechCrunch>
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Chunking de Texto

O `text_vectorizer.py` divide automaticamente textos grandes:

```python
# Configura√ß√£o padr√£o
max_chunk_length = 1000  # caracteres
overlap = 100           # sobreposi√ß√£o entre chunks
```

### Limite da API Gemini

```python
# Texto √© limitado para evitar erros da API
max_text_length = 2048  # caracteres
```

## üêõ Solu√ß√£o de Problemas

### WebScraper

1. **Erro de autentica√ß√£o**: Verifique credenciais no `.env`
2. **MCP n√£o inicializa**: Certifique-se que `npx` est√° instalado
3. **Nenhum site encontrado**: Cadastre sites no modelo `Site` via admin
4. **Timeout na API**: Sites podem estar offline

### Text Vectorizer  

1. **API Key inv√°lida**: Verifique `GOOGLE_GEMINI_API_KEY`
2. **Nenhum JSON encontrado**: Execute `webscraper.py` primeiro
3. **Erro de banco**: Verifique se modelo `Embedding` foi migrado
4. **Quota excedida**: Aguarde reset da quota da API Gemini

### Logs Espec√≠ficos

```bash
# Webscraper
grep "ERROR" scraping/scraping.log

# Vectorizer
grep "ERROR" scraping/vectorizer.log

# Ver estat√≠sticas
grep "Total processado" scraping/vectorizer.log
```

## üöÄ Automa√ß√£o (Futuro)

### Crontab para Execu√ß√£o Autom√°tica

```bash
# Adicionar ao crontab (execu√ß√£o di√°ria √†s 02:00)
0 2 * * * cd /path/to/project && python scraping/webscraper.py && python scraping/text_vectorizer.py
```

### Webhook para Sites

```python
# Futuro: Trigger autom√°tico quando novos sites s√£o cadastrados
@receiver(post_save, sender=Site)
def trigger_scraping_on_new_site(sender, instance, created, **kwargs):
    if created:
        # Executar web scraping automaticamente
        pass
```

## üí° Melhorias Futuras

- **Scraping incremental**: S√≥ buscar conte√∫do novo
- **M√∫ltiplas fontes MCP**: Integrar outros provedores
- **Scheduling avan√ßado**: Diferentes frequ√™ncias por categoria
- **An√°lise de sentimento**: Adicionar an√°lise aos metadados
- **Detec√ß√£o de idioma**: Processar apenas conte√∫do portugu√™s

---

**Integrado ao sistema UniPost API ü§ñ - Web Scraping + IA em perfeita harmonia!**